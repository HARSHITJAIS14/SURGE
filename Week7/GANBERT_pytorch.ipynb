{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crux82/ganbert-pytorch/blob/main/GANBERT_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUpqAwtN8rTA"
      },
      "source": [
        "# GAN-BERT (in Pytorch and compatible with HuggingFace)\n",
        "\n",
        "This is a Pytorch (+ **Huggingface** transformers) implementation of the GAN-BERT model from https://github.com/crux82/ganbert. While the original GAN-BERT was an extension of BERT, this implementation can be adapted to several architectures, ranging from Roberta to Albert!\n",
        "\n",
        "**NOTE**: given that this implementation is different from the original one in Tensorflow, some results can be slighty different.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0m5KR34gmRH"
      },
      "source": [
        "Let's GO!\n",
        "\n",
        "Required Imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1 — installs (run once at the top)\n",
        "# !pip install transformers\n",
        "# (If you need a specific torch build, uncomment & adjust the following)\n",
        "# %pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 \\\n",
        "#     -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# %pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIqpm34x2rms",
        "outputId": "b0205d19-dff1-4967-d003-990c3c5c8164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using PyTorch 2.7.1+cu126 on device NVIDIA TITAN RTX\n"
          ]
        }
      ],
      "source": [
        "# Cell 2 — imports & seed setup\n",
        "import torch\n",
        "import io\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "import torch.nn as nn\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    # …or whatever classes you need\n",
        ")\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "print(f\"Using PyTorch {torch.__version__} on device {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeZgRup520II",
        "outputId": "5b8d1039-e1e6-4712-e77b-e1e9f1b9fbcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 4 GPU(s) available.\n",
            "We will use the GPU: NVIDIA TITAN RTX\n"
          ]
        }
      ],
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    #\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU3ns8Ic7I-h"
      },
      "source": [
        "### Input Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw0HC_hU3FUy",
        "outputId": "6ae87fcf-ed0b-4c78-b9aa-7d86d80cb933"
      },
      "outputs": [],
      "source": [
        "#--------------------------------\n",
        "#  Transformer parameters\n",
        "#--------------------------------\n",
        "max_seq_length = 64\n",
        "batch_size = 64\n",
        "\n",
        "#--------------------------------\n",
        "#  GAN-BERT specific parameters\n",
        "#--------------------------------\n",
        "# number of hidden layers in the generator, \n",
        "# each of the size of the output space\n",
        "num_hidden_layers_g = 1; \n",
        "# number of hidden layers in the discriminator, \n",
        "# each of the size of the input space\n",
        "num_hidden_layers_d = 1; \n",
        "# size of the generator's input noisy vectors\n",
        "noise_size = 100\n",
        "# dropout to be applied to discriminator's input vectors\n",
        "out_dropout_rate = 0.2\n",
        "\n",
        "# Replicate labeled data to balance poorly represented datasets, \n",
        "# e.g., less than 1% of labeled material\n",
        "apply_balance = True\n",
        "\n",
        "#--------------------------------\n",
        "#  Optimization parameters\n",
        "#--------------------------------\n",
        "learning_rate_discriminator = 5e-5\n",
        "learning_rate_generator = 5e-5\n",
        "epsilon = 1e-8\n",
        "num_train_epochs = 10\n",
        "multi_gpu = True\n",
        "# Scheduler\n",
        "apply_scheduler = False\n",
        "warmup_proportion = 0.1\n",
        "# Print\n",
        "print_each_n_step = 10\n",
        "\n",
        "#--------------------------------\n",
        "#  Adopted Tranformer model\n",
        "#--------------------------------\n",
        "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
        "# (or add) transformer models compatible with GAN\n",
        "\n",
        "model_name = \"bert-base-cased\"\n",
        "#model_name = \"bert-base-uncased\"\n",
        "#model_name = \"roberta-base\"\n",
        "#model_name = \"albert-base-v2\"\n",
        "#model_name = \"xlm-roberta-base\"\n",
        "#model_name = \"amazon/bort\"\n",
        "\n",
        "#--------------------------------\n",
        "#  Retrieve the TREC QC Dataset\n",
        "#--------------------------------\n",
        "# ! git clone https://github.com/crux82/ganbert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "df=pd.read_csv(\"/home/jivnesh/Harshit_Surge/dataset/sampled_train.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data=pd.DataFrame()\n",
        "data[\"models\"]=df[\"model\"].apply(lambda x: \"ai\" if x!= \"human\" else \"human\")\n",
        "data['text']=df['title']+\" \"+df['generation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>models</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>human</td>\n",
              "      <td>My mom is shaming me for wanting to get off of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ai</td>\n",
              "      <td>DAE feel like their dreams make up memories? A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ai</td>\n",
              "      <td>FBI agent colludes with analyst An FBI agent s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>human</td>\n",
              "      <td>Towards the effectiveness of Deep Convolutiona...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>human</td>\n",
              "      <td>Helen  Helen tells the story of a young orphan...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  models                                               text\n",
              "0  human  My mom is shaming me for wanting to get off of...\n",
              "1     ai  DAE feel like their dreams make up memories? A...\n",
              "2     ai  FBI agent colludes with analyst An FBI agent s...\n",
              "3  human  Towards the effectiveness of Deep Convolutiona...\n",
              "4  human  Helen  Helen tells the story of a young orphan..."
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_binary_examples(df):\n",
        "    return list(zip(df[\"text\"], df[\"models\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "examples = get_binary_examples(data)\n",
        "label_masks = [True] * len(examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6Q5jzVioTHb"
      },
      "source": [
        "Load the Tranformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "22cde8fbae4e49af993e33f3f2d9a28e",
            "2c671428c4df4355a7a53c71e9bd14ee",
            "44660941ffcc44beae72a1974d458583",
            "afdbd837001c4c74b5ac332ca061ef3a",
            "b489404bf53b4fab9bdb1c0c79a33008",
            "2bfb43b9e8604cbf8ebfd162267f1b8a",
            "4f556917850542da8508f9f839cae9bc",
            "28f045edfa48462fa96a94cffd3b143f",
            "14a6abbb244b41f89626a37640c63118",
            "0dccd0ab28c34880be92b35aa05e6ffb",
            "bf127a02cf6647aba4035c5cbfadc378",
            "098f203f1209452a9d4192af92da7057",
            "ec76cfd2d1da498e9b66885ff1be46b3",
            "b94243bfea7e4441b809b38c7cecd875",
            "0097aa33393342cd99be3dcc30edc5a0",
            "293c4d8660f546f79b43e0dc63250c2f",
            "9fc9479d78e442db91360de7f45b6d7f",
            "14bf2ab82bfc45bd8a3b93f2ab9aa656",
            "a56008dc6c6b41c3850611cc15fb6ea8",
            "504e8c3a107e4e04b51df39e1b3c584e",
            "1f25240f5457445795af70e20e5903f9",
            "5110d1cb9a7547f49244113dc5dd8321",
            "d3a13b7869354881ac7b7887e05d56a7",
            "4d22913664fb4cdbb38e217d4197601d",
            "870fe8f58bb24a47b6a98fa0eed0ebf5",
            "193a5a054d6f4a319842820c4c308322",
            "5a478b81997c4263a60d938447232fd9",
            "5fbebd67d40e470e8a75a4b5b540bbdf",
            "d8b619dff35e4f8cabf586221ad8c962",
            "d7bef2816920414f99a5c9cc676ec254",
            "b465cf9004f549ffa85444bfa16ee4c2",
            "748d5c1fb00e4d91809003527319a9f6",
            "7d501eb3d36e48128a5232879141b281",
            "547d7329b8554b7bb8ae51d61a5e41f8",
            "81b6bafd3fc248afa76cf463f2cb8ab8",
            "bac8f28c84144450b24bbc97fa4860db",
            "30e0829529874db1802f411f72f3d76a",
            "83dd72fbb4204fefb951b3800d73a8d2",
            "f9390d4fc9b147729f1ef5ea85d03774",
            "ddf20490dc874352aa7df35f07a60bcc",
            "c0f0d9a0d4f4440e81e5a6d5a2a3b4ab",
            "f41b83ccf26c40ed88ca6eaf49e09de5",
            "be6339f6256d4b579c53ef8b430ecb25",
            "b79fef6d585843c0a4d885edb2d01ebd"
          ]
        },
        "id": "gxghkkZq3Gbn",
        "outputId": "a4a5afd0-1b6c-4c2d-e3eb-7ced49df4e33"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "transformer = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd_ixn5qn_zV"
      },
      "source": [
        "Function required to load the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBhaW5vBfR6B"
      },
      "source": [
        "Functions required to convert examples into Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fmKL5AD7I4Zg"
      },
      "outputs": [],
      "source": [
        "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n",
        "  '''\n",
        "  Generate a Dataloader given the input examples, eventually masked if they are \n",
        "  to be considered NOT labeled.\n",
        "  '''\n",
        "  examples = []\n",
        "\n",
        "  # Count the percentage of labeled examples  \n",
        "  num_labeled_examples = 0\n",
        "  for label_mask in label_masks:\n",
        "    if label_mask: \n",
        "      num_labeled_examples += 1\n",
        "  label_mask_rate = num_labeled_examples/len(input_examples)\n",
        "\n",
        "  # if required it applies the balance\n",
        "  for index, ex in enumerate(input_examples): \n",
        "    if label_mask_rate == 1 or not balance_label_examples:\n",
        "      examples.append((ex, label_masks[index]))\n",
        "    else:\n",
        "      # IT SIMULATE A LABELED EXAMPLE\n",
        "      if label_masks[index]:\n",
        "        balance = int(1/label_mask_rate)\n",
        "        balance = int(math.log(balance,2))\n",
        "        if balance < 1:\n",
        "          balance = 1\n",
        "        for b in range(0, int(balance)):\n",
        "          examples.append((ex, label_masks[index]))\n",
        "      else:\n",
        "        examples.append((ex, label_masks[index]))\n",
        "  \n",
        "  #-----------------------------------------------\n",
        "  # Generate input examples to the Transformer\n",
        "  #-----------------------------------------------\n",
        "  input_ids = []\n",
        "  input_mask_array = []\n",
        "  label_mask_array = []\n",
        "  label_id_array = []\n",
        "\n",
        "  # Tokenization \n",
        "  for (text, label_mask) in examples:\n",
        "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
        "    input_ids.append(encoded_sent)\n",
        "    label_id_array.append(label_map[text[1]])\n",
        "    label_mask_array.append(label_mask)\n",
        "  \n",
        "  # Attention to token (to ignore padded input wordpieces)\n",
        "  for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
        "    input_mask_array.append(att_mask)\n",
        "  # Convertion to Tensor\n",
        "  input_ids = torch.tensor(input_ids) \n",
        "  input_mask_array = torch.tensor(input_mask_array)\n",
        "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
        "  label_mask_array = torch.tensor(label_mask_array)\n",
        "\n",
        "  # Building the TensorDataset\n",
        "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
        "\n",
        "  if do_shuffle:\n",
        "    sampler = RandomSampler\n",
        "  else:\n",
        "    sampler = SequentialSampler\n",
        "\n",
        "  # Building the DataLoader\n",
        "  return DataLoader(\n",
        "              dataset,  # The training samples.\n",
        "              sampler = sampler(dataset), \n",
        "              batch_size = batch_size) # Trains with this batch size.\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do3O-VeefT3g"
      },
      "source": [
        "Convert the input examples into DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4c-nsMXlKX-D"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "df_human = data[data.models == \"human\"]\n",
        "df_ai    = data[data.models ==\"ai\" ]\n",
        "# 3) Sample for train and test\n",
        "#   - Train: 10 000 human + 10 000 ai = 20 000\n",
        "#   - Test :  2 000 human +  2 000 ai =  4 000\n",
        "train_human = df_human.sample(10000, random_state=42)\n",
        "train_ai    = df_ai.sample(   10000, random_state=42)\n",
        "test_human  = df_human.drop(train_human.index).sample(2000, random_state=42)\n",
        "test_ai     = df_ai   .drop(train_ai   .index).sample(2000, random_state=42)\n",
        "\n",
        "train_df = pd.concat([train_human, train_ai], axis=0)\n",
        "test_df  = pd.concat([test_human,  test_ai ], axis=0)\n",
        "# 4) Shuffle within each split\n",
        "train_df = shuffle(train_df, random_state=42).reset_index(drop=True)\n",
        "test_df  = shuffle(test_df,  random_state=42).reset_index(drop=True)\n",
        "label_map = {\"human\":0, \"ai\":1}\n",
        "label_list = [\"human\",\"ai\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3772421/1314345009.py:54: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
            "  label_mask_array = torch.tensor(label_mask_array)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "→ train: 20000 examples ({'ai': 10000, 'human': 10000})\n",
            "→ test : 4000 examples ({'human': 2000, 'ai': 2000})\n"
          ]
        }
      ],
      "source": [
        "train_examples = list(zip(\n",
        "    train_df[\"text\"].tolist(),\n",
        "    train_df[\"models\"].tolist()\n",
        "))\n",
        "\n",
        "test_examples  = list(zip(\n",
        "    test_df[\"text\"].tolist(),\n",
        "    test_df[\"models\"].tolist()\n",
        "))\n",
        "\n",
        "\n",
        "# 6) Create masks (all True, since every example is labeled)\n",
        "train_label_masks = np.ones(len(train_examples), dtype=bool)\n",
        "test_label_masks  = np.ones(len(test_examples),  dtype=bool)\n",
        "\n",
        "# 7) Build your DataLoaders\n",
        "train_dataloader = generate_data_loader(\n",
        "    train_examples,\n",
        "    train_label_masks,\n",
        "    label_map,\n",
        "    do_shuffle=True,\n",
        "    balance_label_examples=False\n",
        ")\n",
        "\n",
        "test_dataloader = generate_data_loader(\n",
        "    test_examples,\n",
        "    test_label_masks,\n",
        "    label_map,\n",
        "    do_shuffle=False,\n",
        "    balance_label_examples=False\n",
        ")\n",
        "\n",
        "print(f\"→ train: {len(train_examples)} examples ({train_df.models.value_counts().to_dict()})\")\n",
        "print(f\"→ test : {len(test_examples)} examples ({test_df.models.value_counts().to_dict()})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ihcw3vquaQm"
      },
      "source": [
        "We define the Generator and Discriminator as discussed in https://www.aclweb.org/anthology/2020.acl-main.191/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "18kY64-n3I6y"
      },
      "outputs": [],
      "source": [
        "#------------------------------\n",
        "#   The Generator as in \n",
        "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
        "#   https://github.com/crux82/ganbert\n",
        "#------------------------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n",
        "        super(Generator, self).__init__()\n",
        "        layers = []\n",
        "        hidden_sizes = [noise_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, noise):\n",
        "        output_rep = self.layers(noise)\n",
        "        return output_rep\n",
        "\n",
        "#------------------------------\n",
        "#   The Discriminator\n",
        "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
        "#   https://github.com/crux82/ganbert\n",
        "#------------------------------\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
        "        layers = []\n",
        "        hidden_sizes = [input_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        self.layers = nn.Sequential(*layers) #per il flatten\n",
        "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, input_rep):\n",
        "        input_rep = self.input_dropout(input_rep)\n",
        "        last_rep = self.layers(input_rep)\n",
        "        logits = self.logit(last_rep)\n",
        "        probs = self.softmax(logits)\n",
        "        return last_rep, logits, probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uje9s2zQunFc"
      },
      "source": [
        "We instantiate the Discriminator and Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "4\n",
            "NVIDIA TITAN RTX\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "Ylz5rvqE3U2S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoConfig\n",
        "\n",
        "# # Check CUDA availability and set up devices\n",
        "# if not torch.cuda.is_available():\n",
        "#     raise RuntimeError(\"CUDA is not available\")\n",
        "\n",
        "# # Set up device configuration\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# n_gpu = torch.cuda.device_count()\n",
        "# print(f\"Available GPUs: {n_gpu}\")\n",
        "\n",
        "# Get transformer config\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "hidden_size = int(config.hidden_size)\n",
        "\n",
        "# Define hidden layers\n",
        "hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
        "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
        "\n",
        "# Instantiate models\n",
        "generator = Generator(\n",
        "    noise_size=noise_size, \n",
        "    output_size=hidden_size, \n",
        "    hidden_sizes=hidden_levels_g, \n",
        "    dropout_rate=out_dropout_rate\n",
        ")\n",
        "\n",
        "discriminator = Discriminator(\n",
        "    input_size=hidden_size, \n",
        "    hidden_sizes=hidden_levels_d, \n",
        "    num_labels=len(label_list), \n",
        "    dropout_rate=out_dropout_rate\n",
        ")\n",
        "\n",
        "# # Move models to primary device first\n",
        "# transformer = transformer.to(device)\n",
        "# generator = generator.to(device)\n",
        "# discriminator = discriminator.to(device)\n",
        "\n",
        "# # Apply DataParallel for multi-GPU training\n",
        "# if n_gpu > 1:\n",
        "#     print(f\"Using {n_gpu} GPUs for training\")\n",
        "#     transformer = nn.DataParallel(transformer)\n",
        "#     generator = nn.DataParallel(generator) \n",
        "#     discriminator = nn.DataParallel(discriminator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n",
            "Available GPUs: 4\n",
            "Using DataParallel with 4 GPUs\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Setup device configuration - ALWAYS use cuda:0 as primary\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Available GPUs: {n_gpu}\")\n",
        "\n",
        "# Move ALL models to the PRIMARY device first\n",
        "transformer = transformer.to(device)\n",
        "generator = generator.to(device)\n",
        "discriminator = discriminator.to(device)\n",
        "\n",
        "# Apply DataParallel to ALL models consistently\n",
        "if n_gpu > 1:\n",
        "    print(f\"Using DataParallel with {n_gpu} GPUs\")\n",
        "    transformer = nn.DataParallel(transformer)\n",
        "    generator = nn.DataParallel(generator)\n",
        "    discriminator = nn.DataParallel(discriminator)\n",
        "    \n",
        "# Model parameters for optimizers (handle DataParallel)\n",
        "if n_gpu > 1:\n",
        "    transformer_vars = list(transformer.module.parameters())\n",
        "    d_vars = transformer_vars + list(discriminator.module.parameters())\n",
        "    g_vars = list(generator.module.parameters())\n",
        "else:\n",
        "    transformer_vars = list(transformer.parameters())\n",
        "    d_vars = transformer_vars + list(discriminator.parameters())\n",
        "    g_vars = list(generator.parameters())\n",
        "\n",
        "# Create optimizers\n",
        "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
        "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG3qzp2-usZE"
      },
      "source": [
        "Let's go with the training procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhqylHGK3Va4",
        "outputId": "726efd06-d8de-4a45-a7bb-f186994a6b2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======== Epoch 1 / 10 ========\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:08.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:12.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:16.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:20.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:23.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:27.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:31.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:35.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:39.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:43.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:46.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:50.\n",
            "  Batch   140  of    313.    Elapsed: 0:00:54.\n",
            "  Batch   150  of    313.    Elapsed: 0:00:58.\n",
            "  Batch   160  of    313.    Elapsed: 0:01:01.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:05.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:09.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:13.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:17.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:21.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:24.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:28.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:32.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:36.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:40.\n",
            "  Batch   270  of    313.    Elapsed: 0:01:44.\n",
            "  Batch   280  of    313.    Elapsed: 0:01:47.\n",
            "  Batch   290  of    313.    Elapsed: 0:01:51.\n",
            "  Batch   300  of    313.    Elapsed: 0:01:55.\n",
            "  Batch   310  of    313.    Elapsed: 0:01:59.\n",
            "\n",
            "  Average training loss generetor: 0.708\n",
            "  Average training loss discriminator: 1.009\n",
            "  Training epcoh took: 0:02:00\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.920\n",
            "  Test Loss: 0.224\n",
            "  Test took: 0:00:08\n",
            "======== Epoch 2 / 10 ========\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:08.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:11.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:15.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:19.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:23.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:27.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:31.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:35.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:39.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:43.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:47.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:51.\n",
            "  Batch   140  of    313.    Elapsed: 0:00:55.\n",
            "  Batch   150  of    313.    Elapsed: 0:00:58.\n",
            "  Batch   160  of    313.    Elapsed: 0:01:03.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:07.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:11.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:15.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:19.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:23.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:27.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:31.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:35.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:39.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:43.\n",
            "  Batch   270  of    313.    Elapsed: 0:01:47.\n",
            "  Batch   280  of    313.    Elapsed: 0:01:51.\n",
            "  Batch   290  of    313.    Elapsed: 0:01:55.\n",
            "  Batch   300  of    313.    Elapsed: 0:01:59.\n",
            "  Batch   310  of    313.    Elapsed: 0:02:04.\n",
            "\n",
            "  Average training loss generetor: 0.700\n",
            "  Average training loss discriminator: 0.818\n",
            "  Training epcoh took: 0:02:05\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.932\n",
            "  Test Loss: 0.206\n",
            "  Test took: 0:00:07\n",
            "======== Epoch 3 / 10 ========\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:08.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:13.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:17.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:21.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:25.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:29.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:33.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:37.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:42.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:46.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:50.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:54.\n",
            "  Batch   140  of    313.    Elapsed: 0:00:59.\n",
            "  Batch   150  of    313.    Elapsed: 0:01:03.\n",
            "  Batch   160  of    313.    Elapsed: 0:01:07.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:11.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:15.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:19.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:24.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:28.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:32.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:36.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:40.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:45.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:49.\n",
            "  Batch   270  of    313.    Elapsed: 0:01:53.\n",
            "  Batch   280  of    313.    Elapsed: 0:01:57.\n",
            "  Batch   290  of    313.    Elapsed: 0:02:02.\n",
            "  Batch   300  of    313.    Elapsed: 0:02:06.\n",
            "  Batch   310  of    313.    Elapsed: 0:02:10.\n",
            "\n",
            "  Average training loss generetor: 0.699\n",
            "  Average training loss discriminator: 0.770\n",
            "  Training epcoh took: 0:02:11\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.899\n",
            "  Test Loss: 0.380\n",
            "  Test took: 0:00:07\n",
            "======== Epoch 4 / 10 ========\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:09.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:13.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:17.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:21.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:26.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:30.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:34.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:39.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:43.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:47.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:51.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:56.\n",
            "  Batch   140  of    313.    Elapsed: 0:01:00.\n",
            "  Batch   150  of    313.    Elapsed: 0:01:04.\n",
            "  Batch   160  of    313.    Elapsed: 0:01:09.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:13.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:17.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:22.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:26.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:30.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:35.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:39.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:43.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:47.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:52.\n",
            "  Batch   270  of    313.    Elapsed: 0:01:56.\n",
            "  Batch   280  of    313.    Elapsed: 0:02:00.\n",
            "  Batch   290  of    313.    Elapsed: 0:02:05.\n",
            "  Batch   300  of    313.    Elapsed: 0:02:09.\n",
            "  Batch   310  of    313.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss generetor: 0.698\n",
            "  Average training loss discriminator: 0.751\n",
            "  Training epcoh took: 0:02:14\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.926\n",
            "  Test Loss: 0.295\n",
            "  Test took: 0:00:08\n",
            "======== Epoch 5 / 10 ========\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:09.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:13.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:18.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:22.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:26.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:35.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:39.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:44.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:48.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:53.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:57.\n",
            "  Batch   140  of    313.    Elapsed: 0:01:01.\n",
            "  Batch   150  of    313.    Elapsed: 0:01:06.\n",
            "  Batch   160  of    313.    Elapsed: 0:01:10.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:15.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:19.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:24.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:28.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:32.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:37.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:41.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:46.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:50.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:54.\n",
            "  Batch   270  of    313.    Elapsed: 0:01:59.\n",
            "  Batch   280  of    313.    Elapsed: 0:02:03.\n",
            "  Batch   290  of    313.    Elapsed: 0:02:08.\n",
            "  Batch   300  of    313.    Elapsed: 0:02:12.\n",
            "  Batch   310  of    313.    Elapsed: 0:02:17.\n",
            "\n",
            "  Average training loss generetor: 0.697\n",
            "  Average training loss discriminator: 0.729\n",
            "  Training epcoh took: 0:02:18\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.928\n",
            "  Test Loss: 0.267\n",
            "  Test took: 0:00:08\n",
            "======== Epoch 6 / 10 ========\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:09.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:13.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:18.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:22.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:26.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:35.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:40.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:44.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:49.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:53.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:58.\n",
            "  Batch   140  of    313.    Elapsed: 0:01:02.\n",
            "  Batch   150  of    313.    Elapsed: 0:01:06.\n",
            "  Batch   160  of    313.    Elapsed: 0:01:11.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:15.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:20.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:24.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:29.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:33.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:38.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:42.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:47.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:51.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:55.\n",
            "  Batch   270  of    313.    Elapsed: 0:02:00.\n",
            "  Batch   280  of    313.    Elapsed: 0:02:04.\n",
            "  Batch   290  of    313.    Elapsed: 0:02:09.\n",
            "  Batch   300  of    313.    Elapsed: 0:02:13.\n",
            "  Batch   310  of    313.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss generetor: 0.697\n",
            "  Average training loss discriminator: 0.724\n",
            "  Training epcoh took: 0:02:19\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.921\n",
            "  Test Loss: 0.350\n",
            "  Test took: 0:00:08\n",
            "======== Epoch 7 / 10 ========\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:09.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:13.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:18.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:22.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:27.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:35.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:40.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:44.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:49.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:53.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:58.\n",
            "  Batch   140  of    313.    Elapsed: 0:01:02.\n",
            "  Batch   150  of    313.    Elapsed: 0:01:07.\n",
            "  Batch   160  of    313.    Elapsed: 0:01:11.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:16.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:20.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:24.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:29.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:33.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:38.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:42.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:47.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:51.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:56.\n",
            "  Batch   270  of    313.    Elapsed: 0:02:00.\n",
            "  Batch   280  of    313.    Elapsed: 0:02:05.\n",
            "  Batch   290  of    313.    Elapsed: 0:02:09.\n",
            "  Batch   300  of    313.    Elapsed: 0:02:13.\n",
            "  Batch   310  of    313.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss generetor: 0.697\n",
            "  Average training loss discriminator: 0.723\n",
            "  Training epcoh took: 0:02:19\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.938\n",
            "  Test Loss: 0.282\n",
            "  Test took: 0:00:08\n",
            "======== Epoch 8 / 10 ========\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:09.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:13.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:18.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:22.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:27.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:36.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:40.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:44.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:49.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:53.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:58.\n",
            "  Batch   140  of    313.    Elapsed: 0:01:02.\n",
            "  Batch   150  of    313.    Elapsed: 0:01:07.\n",
            "  Batch   160  of    313.    Elapsed: 0:01:11.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:16.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:20.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:24.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:29.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:33.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:38.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:42.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:47.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:51.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:56.\n",
            "  Batch   270  of    313.    Elapsed: 0:02:00.\n",
            "  Batch   280  of    313.    Elapsed: 0:02:05.\n",
            "  Batch   290  of    313.    Elapsed: 0:02:09.\n",
            "  Batch   300  of    313.    Elapsed: 0:02:14.\n",
            "  Batch   310  of    313.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss generetor: 0.696\n",
            "  Average training loss discriminator: 0.715\n",
            "  Training epcoh took: 0:02:19\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.934\n",
            "  Test Loss: 0.403\n",
            "  Test took: 0:00:08\n",
            "======== Epoch 9 / 10 ========\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:09.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:13.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:18.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:22.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:27.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:35.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:40.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:44.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:49.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:53.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:58.\n",
            "  Batch   140  of    313.    Elapsed: 0:01:02.\n",
            "  Batch   150  of    313.    Elapsed: 0:01:06.\n",
            "  Batch   160  of    313.    Elapsed: 0:01:11.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:15.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:20.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:24.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:29.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:33.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:38.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:42.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:47.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:51.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:55.\n",
            "  Batch   270  of    313.    Elapsed: 0:02:00.\n",
            "  Batch   280  of    313.    Elapsed: 0:02:04.\n",
            "  Batch   290  of    313.    Elapsed: 0:02:09.\n",
            "  Batch   300  of    313.    Elapsed: 0:02:13.\n",
            "  Batch   310  of    313.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss generetor: 0.696\n",
            "  Average training loss discriminator: 0.717\n",
            "  Training epcoh took: 0:02:19\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.933\n",
            "  Test Loss: 0.311\n",
            "  Test took: 0:00:08\n",
            "======== Epoch 10 / 10 ========\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:09.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:13.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:18.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:22.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:27.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:36.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:40.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:44.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:49.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:53.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:58.\n",
            "  Batch   140  of    313.    Elapsed: 0:01:02.\n",
            "  Batch   150  of    313.    Elapsed: 0:01:07.\n",
            "  Batch   160  of    313.    Elapsed: 0:01:11.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:16.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:20.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:25.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:29.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:34.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:38.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:43.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:47.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:52.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:56.\n",
            "  Batch   270  of    313.    Elapsed: 0:02:00.\n",
            "  Batch   280  of    313.    Elapsed: 0:02:05.\n",
            "  Batch   290  of    313.    Elapsed: 0:02:09.\n",
            "  Batch   300  of    313.    Elapsed: 0:02:14.\n",
            "  Batch   310  of    313.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss generetor: 0.696\n",
            "  Average training loss discriminator: 0.710\n",
            "  Training epcoh took: 0:02:20\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.903\n",
            "  Test Loss: 0.587\n",
            "  Test took: 0:00:08\n"
          ]
        }
      ],
      "source": [
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "#models parameters\n",
        "# transformer_vars = [i for i in transformer.parameters()]\n",
        "# d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
        "# g_vars = [v for v in generator.parameters()]\n",
        "\n",
        "# #optimizer\n",
        "# dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
        "# gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
        "\n",
        "# #scheduler\n",
        "# if apply_scheduler:\n",
        "#   num_train_examples = len(train_examples)\n",
        "#   num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
        "#   num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "#   scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
        "#                                            num_warmup_steps = num_warmup_steps)\n",
        "#   scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
        "#                                            num_warmup_steps = num_warmup_steps)\n",
        "num_train_epochs = 20\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, num_train_epochs):\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n",
        "    \n",
        "    # Set training mode\n",
        "    transformer.train()\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    \n",
        "    tr_g_loss = 0\n",
        "    tr_d_loss = 0\n",
        "    t0 = time.time()\n",
        "    \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % print_each_n_step == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Move ALL batch data to PRIMARY device (cuda:0)\n",
        "        b_input_ids = batch[0].to(device, non_blocking=True)\n",
        "        b_input_mask = batch[1].to(device, non_blocking=True)\n",
        "        b_labels = batch[2].to(device, non_blocking=True)\n",
        "        b_label_mask = batch[3].to(device, non_blocking=True)\n",
        "\n",
        "        real_batch_size = b_input_ids.shape[0]\n",
        "        \n",
        "        # Clear gradients\n",
        "        gen_optimizer.zero_grad()\n",
        "        dis_optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass through transformer\n",
        "        # DataParallel automatically handles device distribution\n",
        "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "        hidden_states = model_outputs[-1]\n",
        "        \n",
        "        # Generate noise on PRIMARY device\n",
        "        noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n",
        "        gen_rep = generator(noise)\n",
        "\n",
        "        # Concatenate - both tensors should be on same device after DataParallel\n",
        "        discriminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n",
        "        features, logits, probs = discriminator(discriminator_input)\n",
        "\n",
        "        # Rest of your loss calculation and optimization code...\n",
        "        # (Keep the same loss calculation logic you have)\n",
        "\n",
        "\n",
        "        # Finally, we separate the discriminator's output for the real and fake\n",
        "        # data\n",
        "        features_list = torch.split(features, real_batch_size)\n",
        "        D_real_features = features_list[0]\n",
        "        D_fake_features = features_list[1]\n",
        "      \n",
        "        logits_list = torch.split(logits, real_batch_size)\n",
        "        D_real_logits = logits_list[0]\n",
        "        D_fake_logits = logits_list[1]\n",
        "        \n",
        "        probs_list = torch.split(probs, real_batch_size)\n",
        "        D_real_probs = probs_list[0]\n",
        "        D_fake_probs = probs_list[1]\n",
        "\n",
        "        #---------------------------------\n",
        "        #  LOSS evaluation\n",
        "        #---------------------------------\n",
        "        # Generator's LOSS estimation\n",
        "        g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
        "        g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
        "        g_loss = g_loss_d + g_feat_reg\n",
        "  \n",
        "        # Disciminator's LOSS estimation\n",
        "        logits = D_real_logits[:,0:-1]\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        # The discriminator provides an output for labeled and unlabeled real data\n",
        "        # so the loss evaluated for unlabeled data is ignored (masked)\n",
        "        label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
        "        per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
        "        per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
        "        labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
        "\n",
        "        # It may be the case that a batch does not contain labeled examples, \n",
        "        # so the \"supervised loss\" in this case is not evaluated\n",
        "        if labeled_example_count == 0:\n",
        "          D_L_Supervised = 0\n",
        "        else:\n",
        "          D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
        "                 \n",
        "        D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
        "        D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
        "        d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
        "\n",
        "        #---------------------------------\n",
        "        #  OPTIMIZATION\n",
        "        #---------------------------------\n",
        "        # Avoid gradient accumulation\n",
        "        gen_optimizer.zero_grad()\n",
        "        dis_optimizer.zero_grad()\n",
        "\n",
        "        # Calculate weigth updates\n",
        "        # retain_graph=True is required since the underlying graph will be deleted after backward\n",
        "        g_loss.backward(retain_graph=True)\n",
        "        d_loss.backward() \n",
        "        \n",
        "        # Apply modifications\n",
        "        gen_optimizer.step()\n",
        "        dis_optimizer.step()\n",
        "\n",
        "        # A detail log of the individual losses\n",
        "        #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
        "        #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
        "        #             g_loss_d, g_feat_reg))\n",
        "\n",
        "        # Save the losses to print them later\n",
        "        tr_g_loss += g_loss.item()\n",
        "        tr_d_loss += d_loss.item()\n",
        "\n",
        "        # Update the learning rate with the scheduler\n",
        "        if apply_scheduler:\n",
        "          scheduler_d.step()\n",
        "          scheduler_g.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
        "    avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n",
        "    print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #     TEST ON THE EVALUATION DATASET\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our test set.\n",
        "    print(\"\")\n",
        "    print(\"Running Test...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    transformer.eval() #maybe redundant\n",
        "    discriminator.eval()\n",
        "    generator.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_test_accuracy = 0\n",
        "   \n",
        "    total_test_loss = 0\n",
        "    nb_test_steps = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels_ids = []\n",
        "\n",
        "    #loss\n",
        "    nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in test_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "            hidden_states = model_outputs[-1]\n",
        "            _, logits, probs = discriminator(hidden_states)\n",
        "            ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
        "            filtered_logits = logits[:,0:-1]\n",
        "            # Accumulate the test loss.\n",
        "            total_test_loss += nll_loss(filtered_logits, b_labels)\n",
        "            \n",
        "        # Accumulate the predictions and the input labels\n",
        "        _, preds = torch.max(filtered_logits, 1)\n",
        "        all_preds += preds.detach().cpu()\n",
        "        all_labels_ids += b_labels.detach().cpu()\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "\n",
        "    all_preds = torch.stack(all_preds).numpy()\n",
        "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
        "    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
        "    print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
        "            \n",
        "    # Detailed classification report\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(all_labels_ids, all_preds, target_names=label_list, zero_division=0))\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(all_labels_ids, all_preds))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "    avg_test_loss = avg_test_loss.item()\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    test_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
        "    print(\"  Test took: {:}\".format(test_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss generator': avg_train_loss_g,\n",
        "            'Training Loss discriminator': avg_train_loss_d,\n",
        "            'Valid. Loss': avg_test_loss,\n",
        "            'Valid. Accur.': test_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Test Time': test_time\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDm9NProRB4c",
        "outputId": "2ffebcf1-6b39-4442-88c6-5f72a58a3722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 1, 'Training Loss generator': 0.7080368652892189, 'Training Loss discriminator': 1.0091265285738742, 'Valid. Loss': 0.22438304126262665, 'Valid. Accur.': np.float64(0.92025), 'Training Time': '0:02:00', 'Test Time': '0:00:08'}\n",
            "{'epoch': 2, 'Training Loss generator': 0.700348071206492, 'Training Loss discriminator': 0.8176187507260722, 'Valid. Loss': 0.20562304556369781, 'Valid. Accur.': np.float64(0.93225), 'Training Time': '0:02:05', 'Test Time': '0:00:07'}\n",
            "{'epoch': 3, 'Training Loss generator': 0.6986279137218341, 'Training Loss discriminator': 0.7702838744218357, 'Valid. Loss': 0.38024967908859253, 'Valid. Accur.': np.float64(0.89925), 'Training Time': '0:02:11', 'Test Time': '0:00:07'}\n",
            "{'epoch': 4, 'Training Loss generator': 0.6983037029211514, 'Training Loss discriminator': 0.7514471847790117, 'Valid. Loss': 0.2948113679885864, 'Valid. Accur.': np.float64(0.926), 'Training Time': '0:02:14', 'Test Time': '0:00:08'}\n",
            "{'epoch': 5, 'Training Loss generator': 0.6974214597251087, 'Training Loss discriminator': 0.7291433811187744, 'Valid. Loss': 0.2671601176261902, 'Valid. Accur.': np.float64(0.928), 'Training Time': '0:02:18', 'Test Time': '0:00:08'}\n",
            "{'epoch': 6, 'Training Loss generator': 0.6967657379829846, 'Training Loss discriminator': 0.7242823161256199, 'Valid. Loss': 0.3495733141899109, 'Valid. Accur.': np.float64(0.92125), 'Training Time': '0:02:19', 'Test Time': '0:00:08'}\n",
            "{'epoch': 7, 'Training Loss generator': 0.6971407599342517, 'Training Loss discriminator': 0.7228625212995389, 'Valid. Loss': 0.2823050022125244, 'Valid. Accur.': np.float64(0.9375), 'Training Time': '0:02:19', 'Test Time': '0:00:08'}\n",
            "{'epoch': 8, 'Training Loss generator': 0.6962300422854317, 'Training Loss discriminator': 0.7154770518263308, 'Valid. Loss': 0.4026932418346405, 'Valid. Accur.': np.float64(0.93375), 'Training Time': '0:02:19', 'Test Time': '0:00:08'}\n",
            "{'epoch': 9, 'Training Loss generator': 0.6963959302003391, 'Training Loss discriminator': 0.7165615684308183, 'Valid. Loss': 0.3113085627555847, 'Valid. Accur.': np.float64(0.93275), 'Training Time': '0:02:19', 'Test Time': '0:00:08'}\n",
            "{'epoch': 10, 'Training Loss generator': 0.6956240279605975, 'Training Loss discriminator': 0.7096516598527804, 'Valid. Loss': 0.5866223573684692, 'Valid. Accur.': np.float64(0.903), 'Training Time': '0:02:20', 'Test Time': '0:00:08'}\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:30:58 (h:mm:ss)\n"
          ]
        }
      ],
      "source": [
        "for stat in training_stats:\n",
        "  print(stat)\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using single device: cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fresh transformer type: <class 'transformers.models.bert.modeling_bert.BertModel'>\n",
            "Has config: True\n",
            "Hidden size: 768\n",
            "Number of labels: 2\n",
            "Classifier created successfully on cuda:0\n",
            "Starting baseline transformer training...\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of    313.    Elapsed: 0:00:02.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:06.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:09.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:11.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:13.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:15.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:17.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:20.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:22.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:24.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:27.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:29.\n",
            "  Batch   140  of    313.    Elapsed: 0:00:32.\n",
            "  Batch   150  of    313.    Elapsed: 0:00:34.\n",
            "  Batch   160  of    313.    Elapsed: 0:00:37.\n",
            "  Batch   170  of    313.    Elapsed: 0:00:40.\n",
            "  Batch   180  of    313.    Elapsed: 0:00:42.\n",
            "  Batch   190  of    313.    Elapsed: 0:00:45.\n",
            "  Batch   200  of    313.    Elapsed: 0:00:48.\n",
            "  Batch   210  of    313.    Elapsed: 0:00:51.\n",
            "  Batch   220  of    313.    Elapsed: 0:00:53.\n",
            "  Batch   230  of    313.    Elapsed: 0:00:56.\n",
            "  Batch   240  of    313.    Elapsed: 0:00:59.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:02.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:05.\n",
            "  Batch   270  of    313.    Elapsed: 0:01:08.\n",
            "  Batch   280  of    313.    Elapsed: 0:01:11.\n",
            "  Batch   290  of    313.    Elapsed: 0:01:14.\n",
            "  Batch   300  of    313.    Elapsed: 0:01:17.\n",
            "  Batch   310  of    313.    Elapsed: 0:01:20.\n",
            "  Average training loss: 0.278\n",
            "  Training epoch took: 0:01:21\n",
            "Running Evaluation...\n",
            "  Accuracy: 0.913\n",
            "  Average evaluation loss: 0.204\n",
            "  Evaluation took: 0:00:06\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.96      0.86      0.91      2000\n",
            "          ai       0.88      0.96      0.92      2000\n",
            "\n",
            "    accuracy                           0.91      4000\n",
            "   macro avg       0.92      0.91      0.91      4000\n",
            "weighted avg       0.92      0.91      0.91      4000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1730  270]\n",
            " [  79 1921]]\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of    313.    Elapsed: 0:00:03.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:06.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:10.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:13.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:16.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:19.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:23.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:26.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:29.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:33.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:36.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:39.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:43.\n",
            "  Batch   140  of    313.    Elapsed: 0:00:46.\n",
            "  Batch   150  of    313.    Elapsed: 0:00:49.\n",
            "  Batch   160  of    313.    Elapsed: 0:00:53.\n",
            "  Batch   170  of    313.    Elapsed: 0:00:56.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:00.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:03.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:07.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:10.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:13.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:17.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:20.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:24.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:27.\n",
            "  Batch   270  of    313.    Elapsed: 0:01:31.\n",
            "  Batch   280  of    313.    Elapsed: 0:01:34.\n",
            "  Batch   290  of    313.    Elapsed: 0:01:38.\n",
            "  Batch   300  of    313.    Elapsed: 0:01:42.\n",
            "  Batch   310  of    313.    Elapsed: 0:01:45.\n",
            "  Average training loss: 0.118\n",
            "  Training epoch took: 0:01:46\n",
            "Running Evaluation...\n",
            "  Accuracy: 0.930\n",
            "  Average evaluation loss: 0.208\n",
            "  Evaluation took: 0:00:07\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.95      0.90      0.93      2000\n",
            "          ai       0.91      0.96      0.93      2000\n",
            "\n",
            "    accuracy                           0.93      4000\n",
            "   macro avg       0.93      0.93      0.93      4000\n",
            "weighted avg       0.93      0.93      0.93      4000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1803  197]\n",
            " [  85 1915]]\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:07.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:11.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:14.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:18.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:22.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:25.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:29.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:32.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:36.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:40.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:43.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:47.\n",
            "  Batch   140  of    313.    Elapsed: 0:00:50.\n",
            "  Batch   150  of    313.    Elapsed: 0:00:54.\n",
            "  Batch   160  of    313.    Elapsed: 0:00:58.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:01.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:05.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:09.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:12.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:16.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:20.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:23.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:27.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:31.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:34.\n",
            "  Batch   270  of    313.    Elapsed: 0:01:38.\n",
            "  Batch   280  of    313.    Elapsed: 0:01:41.\n",
            "  Batch   290  of    313.    Elapsed: 0:01:45.\n",
            "  Batch   300  of    313.    Elapsed: 0:01:49.\n",
            "  Batch   310  of    313.    Elapsed: 0:01:52.\n",
            "  Average training loss: 0.068\n",
            "  Training epoch took: 0:01:53\n",
            "Running Evaluation...\n",
            "  Accuracy: 0.903\n",
            "  Average evaluation loss: 0.436\n",
            "  Evaluation took: 0:00:07\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.96      0.84      0.90      2000\n",
            "          ai       0.86      0.96      0.91      2000\n",
            "\n",
            "    accuracy                           0.90      4000\n",
            "   macro avg       0.91      0.90      0.90      4000\n",
            "weighted avg       0.91      0.90      0.90      4000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1683  317]\n",
            " [  72 1928]]\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:07.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:11.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:15.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:18.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:22.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:29.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:33.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:37.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:40.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:44.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:48.\n",
            "  Batch   140  of    313.    Elapsed: 0:00:51.\n",
            "  Batch   150  of    313.    Elapsed: 0:00:55.\n",
            "  Batch   160  of    313.    Elapsed: 0:00:59.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:02.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:06.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:10.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:13.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:17.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:21.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:24.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:28.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:32.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:35.\n",
            "  Batch   270  of    313.    Elapsed: 0:01:39.\n",
            "  Batch   280  of    313.    Elapsed: 0:01:43.\n",
            "  Batch   290  of    313.    Elapsed: 0:01:46.\n",
            "  Batch   300  of    313.    Elapsed: 0:01:50.\n",
            "  Batch   310  of    313.    Elapsed: 0:01:54.\n",
            "  Average training loss: 0.042\n",
            "  Training epoch took: 0:01:55\n",
            "Running Evaluation...\n",
            "  Accuracy: 0.901\n",
            "  Average evaluation loss: 0.506\n",
            "  Evaluation took: 0:00:07\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.97      0.83      0.89      2000\n",
            "          ai       0.85      0.97      0.91      2000\n",
            "\n",
            "    accuracy                           0.90      4000\n",
            "   macro avg       0.91      0.90      0.90      4000\n",
            "weighted avg       0.91      0.90      0.90      4000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1654  346]\n",
            " [  51 1949]]\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:07.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:11.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:15.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:18.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:22.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:30.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:33.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:37.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:41.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:44.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:48.\n",
            "  Batch   140  of    313.    Elapsed: 0:00:52.\n",
            "  Batch   150  of    313.    Elapsed: 0:00:55.\n",
            "  Batch   160  of    313.    Elapsed: 0:00:59.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:03.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:07.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:10.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:14.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:18.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:21.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:25.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:29.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:33.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:36.\n",
            "  Batch   270  of    313.    Elapsed: 0:01:40.\n",
            "  Batch   280  of    313.    Elapsed: 0:01:44.\n",
            "  Batch   290  of    313.    Elapsed: 0:01:47.\n",
            "  Batch   300  of    313.    Elapsed: 0:01:51.\n",
            "  Batch   310  of    313.    Elapsed: 0:01:55.\n",
            "  Average training loss: 0.028\n",
            "  Training epoch took: 0:01:56\n",
            "Running Evaluation...\n",
            "  Accuracy: 0.924\n",
            "  Average evaluation loss: 0.398\n",
            "  Evaluation took: 0:00:07\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.96      0.88      0.92      2000\n",
            "          ai       0.89      0.96      0.93      2000\n",
            "\n",
            "    accuracy                           0.92      4000\n",
            "   macro avg       0.93      0.92      0.92      4000\n",
            "weighted avg       0.93      0.92      0.92      4000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1767  233]\n",
            " [  70 1930]]\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:07.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:11.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:15.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:19.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:22.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:30.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:34.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:37.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:41.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:45.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:48.\n",
            "  Batch   140  of    313.    Elapsed: 0:00:52.\n",
            "  Batch   150  of    313.    Elapsed: 0:00:56.\n",
            "  Batch   160  of    313.    Elapsed: 0:01:00.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:03.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:07.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:11.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:15.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:18.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:22.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:26.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:29.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:33.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:37.\n",
            "  Batch   270  of    313.    Elapsed: 0:01:41.\n",
            "  Batch   280  of    313.    Elapsed: 0:01:44.\n",
            "  Batch   290  of    313.    Elapsed: 0:01:48.\n",
            "  Batch   300  of    313.    Elapsed: 0:01:52.\n",
            "  Batch   310  of    313.    Elapsed: 0:01:56.\n",
            "  Average training loss: 0.019\n",
            "  Training epoch took: 0:01:57\n",
            "Running Evaluation...\n",
            "  Accuracy: 0.925\n",
            "  Average evaluation loss: 0.391\n",
            "  Evaluation took: 0:00:07\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.96      0.88      0.92      2000\n",
            "          ai       0.89      0.97      0.93      2000\n",
            "\n",
            "    accuracy                           0.92      4000\n",
            "   macro avg       0.93      0.92      0.92      4000\n",
            "weighted avg       0.93      0.92      0.92      4000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1764  236]\n",
            " [  65 1935]]\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:07.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:11.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:15.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:19.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:22.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:30.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:34.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:37.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:41.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:45.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:49.\n",
            "  Batch   140  of    313.    Elapsed: 0:00:53.\n",
            "  Batch   150  of    313.    Elapsed: 0:00:56.\n",
            "  Batch   160  of    313.    Elapsed: 0:01:00.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:04.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:08.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:11.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:15.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:19.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:23.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:26.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:30.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:34.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:38.\n",
            "  Batch   270  of    313.    Elapsed: 0:01:41.\n",
            "  Batch   280  of    313.    Elapsed: 0:01:45.\n",
            "  Batch   290  of    313.    Elapsed: 0:01:49.\n",
            "  Batch   300  of    313.    Elapsed: 0:01:53.\n",
            "  Batch   310  of    313.    Elapsed: 0:01:56.\n",
            "  Average training loss: 0.013\n",
            "  Training epoch took: 0:01:57\n",
            "Running Evaluation...\n",
            "  Accuracy: 0.924\n",
            "  Average evaluation loss: 0.486\n",
            "  Evaluation took: 0:00:07\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.97      0.87      0.92      2000\n",
            "          ai       0.89      0.97      0.93      2000\n",
            "\n",
            "    accuracy                           0.92      4000\n",
            "   macro avg       0.93      0.92      0.92      4000\n",
            "weighted avg       0.93      0.92      0.92      4000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1748  252]\n",
            " [  50 1950]]\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:08.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:11.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:15.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:19.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:23.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:30.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:34.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:38.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:41.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:45.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:49.\n",
            "  Batch   140  of    313.    Elapsed: 0:00:53.\n",
            "  Batch   150  of    313.    Elapsed: 0:00:56.\n",
            "  Batch   160  of    313.    Elapsed: 0:01:00.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:04.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:08.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:11.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:15.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:19.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:23.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:27.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:30.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:34.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:38.\n",
            "  Batch   270  of    313.    Elapsed: 0:01:42.\n",
            "  Batch   280  of    313.    Elapsed: 0:01:45.\n",
            "  Batch   290  of    313.    Elapsed: 0:01:49.\n",
            "  Batch   300  of    313.    Elapsed: 0:01:53.\n",
            "  Batch   310  of    313.    Elapsed: 0:01:57.\n",
            "  Average training loss: 0.011\n",
            "  Training epoch took: 0:01:58\n",
            "Running Evaluation...\n",
            "  Accuracy: 0.913\n",
            "  Average evaluation loss: 0.605\n",
            "  Evaluation took: 0:00:07\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.97      0.85      0.91      2000\n",
            "          ai       0.87      0.97      0.92      2000\n",
            "\n",
            "    accuracy                           0.91      4000\n",
            "   macro avg       0.92      0.91      0.91      4000\n",
            "weighted avg       0.92      0.91      0.91      4000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1708  292]\n",
            " [  54 1946]]\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:08.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:11.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:15.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:19.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:23.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:30.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:34.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:38.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:41.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:45.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:49.\n",
            "  Batch   140  of    313.    Elapsed: 0:00:53.\n",
            "  Batch   150  of    313.    Elapsed: 0:00:56.\n",
            "  Batch   160  of    313.    Elapsed: 0:01:00.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:04.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:08.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:12.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:15.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:19.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:23.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:27.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:30.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:34.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:38.\n",
            "  Batch   270  of    313.    Elapsed: 0:01:42.\n",
            "  Batch   280  of    313.    Elapsed: 0:01:45.\n",
            "  Batch   290  of    313.    Elapsed: 0:01:49.\n",
            "  Batch   300  of    313.    Elapsed: 0:01:53.\n",
            "  Batch   310  of    313.    Elapsed: 0:01:57.\n",
            "  Average training loss: 0.010\n",
            "  Training epoch took: 0:01:58\n",
            "Running Evaluation...\n",
            "  Accuracy: 0.927\n",
            "  Average evaluation loss: 0.535\n",
            "  Evaluation took: 0:00:07\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.97      0.88      0.92      2000\n",
            "          ai       0.89      0.97      0.93      2000\n",
            "\n",
            "    accuracy                           0.93      4000\n",
            "   macro avg       0.93      0.93      0.93      4000\n",
            "weighted avg       0.93      0.93      0.93      4000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1768  232]\n",
            " [  63 1937]]\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    10  of    313.    Elapsed: 0:00:04.\n",
            "  Batch    20  of    313.    Elapsed: 0:00:08.\n",
            "  Batch    30  of    313.    Elapsed: 0:00:11.\n",
            "  Batch    40  of    313.    Elapsed: 0:00:15.\n",
            "  Batch    50  of    313.    Elapsed: 0:00:19.\n",
            "  Batch    60  of    313.    Elapsed: 0:00:23.\n",
            "  Batch    70  of    313.    Elapsed: 0:00:26.\n",
            "  Batch    80  of    313.    Elapsed: 0:00:30.\n",
            "  Batch    90  of    313.    Elapsed: 0:00:34.\n",
            "  Batch   100  of    313.    Elapsed: 0:00:38.\n",
            "  Batch   110  of    313.    Elapsed: 0:00:41.\n",
            "  Batch   120  of    313.    Elapsed: 0:00:45.\n",
            "  Batch   130  of    313.    Elapsed: 0:00:49.\n",
            "  Batch   140  of    313.    Elapsed: 0:00:53.\n",
            "  Batch   150  of    313.    Elapsed: 0:00:56.\n",
            "  Batch   160  of    313.    Elapsed: 0:01:00.\n",
            "  Batch   170  of    313.    Elapsed: 0:01:04.\n",
            "  Batch   180  of    313.    Elapsed: 0:01:08.\n",
            "  Batch   190  of    313.    Elapsed: 0:01:12.\n",
            "  Batch   200  of    313.    Elapsed: 0:01:15.\n",
            "  Batch   210  of    313.    Elapsed: 0:01:19.\n",
            "  Batch   220  of    313.    Elapsed: 0:01:23.\n",
            "  Batch   230  of    313.    Elapsed: 0:01:27.\n",
            "  Batch   240  of    313.    Elapsed: 0:01:30.\n",
            "  Batch   250  of    313.    Elapsed: 0:01:34.\n",
            "  Batch   260  of    313.    Elapsed: 0:01:38.\n",
            "  Batch   270  of    313.    Elapsed: 0:01:42.\n",
            "  Batch   280  of    313.    Elapsed: 0:01:45.\n",
            "  Batch   290  of    313.    Elapsed: 0:01:49.\n",
            "  Batch   300  of    313.    Elapsed: 0:01:53.\n",
            "  Batch   310  of    313.    Elapsed: 0:01:57.\n",
            "  Average training loss: 0.009\n",
            "  Training epoch took: 0:01:58\n",
            "Running Evaluation...\n",
            "  Accuracy: 0.941\n",
            "  Average evaluation loss: 0.413\n",
            "  Evaluation took: 0:00:07\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.97      0.91      0.94      2000\n",
            "          ai       0.92      0.97      0.94      2000\n",
            "\n",
            "    accuracy                           0.94      4000\n",
            "   macro avg       0.94      0.94      0.94      4000\n",
            "weighted avg       0.94      0.94      0.94      4000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1828  172]\n",
            " [  65 1935]]\n",
            "Training complete!\n",
            "Total training took 0:19:50\n",
            "\n",
            "Training Statistics:\n",
            "   epoch  Training Loss  Valid. Loss  Valid. Accur. Training Time  \\\n",
            "0      1       0.277978     0.204338       0.913442       0:01:21   \n",
            "1      2       0.117549     0.208016       0.929563       0:01:46   \n",
            "2      3       0.067631     0.435550       0.903026       0:01:53   \n",
            "3      4       0.041985     0.505664       0.901042       0:01:55   \n",
            "4      5       0.027804     0.397899       0.924355       0:01:56   \n",
            "5      6       0.018599     0.391290       0.925099       0:01:57   \n",
            "6      7       0.012676     0.485604       0.924355       0:01:57   \n",
            "7      8       0.011100     0.604815       0.913442       0:01:58   \n",
            "8      9       0.010242     0.534694       0.926587       0:01:58   \n",
            "9     10       0.008646     0.413132       0.940972       0:01:58   \n",
            "\n",
            "  Evaluation Time  \n",
            "0         0:00:06  \n",
            "1         0:00:07  \n",
            "2         0:00:07  \n",
            "3         0:00:07  \n",
            "4         0:00:07  \n",
            "5         0:00:07  \n",
            "6         0:00:07  \n",
            "7         0:00:07  \n",
            "8         0:00:07  \n",
            "9         0:00:07  \n",
            "\n",
            "Baseline Transformer Best Accuracy: 0.941\n",
            "Your GAN Training Best Accuracy: 0.938\n",
            "GAN Improvement: -0.003\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from transformers import AutoModel\n",
        "\n",
        "# Setup device - SINGLE GPU ONLY, no DataParallel\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using single device: {device}\")\n",
        "\n",
        "# COMPLETELY FRESH transformer - replace with your actual model name\n",
        "# Based on your GAN training, this should be the same model you used successfully\n",
        "# model_name = \"roberta-base\"  # Replace with your actual model name from successful GAN training\n",
        "transformer = AutoModel.from_pretrained(model_name)\n",
        "transformer = transformer.to(device)\n",
        "\n",
        "print(f\"Fresh transformer type: {type(transformer)}\")\n",
        "print(f\"Has config: {hasattr(transformer, 'config')}\")\n",
        "\n",
        "# Get config and setup\n",
        "config = transformer.config\n",
        "hidden_size = config.hidden_size\n",
        "num_labels = len(label_list)\n",
        "\n",
        "print(f\"Hidden size: {hidden_size}\")\n",
        "print(f\"Number of labels: {num_labels}\")\n",
        "\n",
        "# Clean classifier implementation with proper pooling\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, transformer_model, hidden_size, num_labels, dropout_rate=0.1):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.transformer = transformer_model\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Get transformer outputs\n",
        "        transformer_outputs = self.transformer(input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = transformer_outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
        "        \n",
        "        # Mean pooling with attention mask\n",
        "        if attention_mask is not None:\n",
        "            # Expand mask to match hidden states dimensions\n",
        "            mask_expanded = attention_mask.unsqueeze(-1).expand_as(hidden_states).float()\n",
        "            # Apply mask and compute mean\n",
        "            sum_embeddings = torch.sum(hidden_states * mask_expanded, dim=1)\n",
        "            sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
        "            pooled_output = sum_embeddings / sum_mask\n",
        "        else:\n",
        "            # Simple mean pooling if no mask\n",
        "            pooled_output = torch.mean(hidden_states, dim=1)\n",
        "        \n",
        "        # Classification\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Create classifier - SINGLE GPU, NO DataParallel\n",
        "classifier_model = TransformerClassifier(\n",
        "    transformer_model=transformer,\n",
        "    hidden_size=hidden_size,\n",
        "    num_labels=num_labels,\n",
        "    dropout_rate=0.1\n",
        ").to(device)\n",
        "\n",
        "print(f\"Classifier created successfully on {device}\")\n",
        "\n",
        "# Setup training - SINGLE GPU\n",
        "optimizer = torch.optim.AdamW(classifier_model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "# Learning rate scheduler\n",
        "if apply_scheduler:\n",
        "    num_train_examples = len(train_examples)\n",
        "    num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
        "    num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "    \n",
        "    from transformers import get_constant_schedule_with_warmup\n",
        "    scheduler = get_constant_schedule_with_warmup(\n",
        "        optimizer, \n",
        "        num_warmup_steps=num_warmup_steps\n",
        "    )\n",
        "\n",
        "print(\"Starting baseline transformer training...\")\n",
        "\n",
        "# Training statistics\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "\n",
        "# Training loop\n",
        "for epoch_i in range(0, num_train_epochs):\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n",
        "    print('Training...')\n",
        "    \n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    successful_batches = 0\n",
        "    classifier_model.train()\n",
        "    \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % print_each_n_step == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "        \n",
        "        # Move batch to device\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        b_label_mask = batch[3].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        logits = classifier_model(b_input_ids, attention_mask=b_input_mask)\n",
        "        \n",
        "        # Calculate loss only for labeled examples\n",
        "        if b_label_mask.sum() > 0:\n",
        "            masked_logits = logits[b_label_mask.bool()]\n",
        "            masked_labels = b_labels[b_label_mask.bool()]\n",
        "            loss = criterion(masked_logits, masked_labels)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(classifier_model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            if apply_scheduler:\n",
        "                scheduler.step()\n",
        "            \n",
        "            total_train_loss += loss.item()\n",
        "            successful_batches += 1\n",
        "        else:\n",
        "            continue\n",
        "    \n",
        "    # Calculate average training loss\n",
        "    if successful_batches > 0:\n",
        "        avg_train_loss = total_train_loss / successful_batches\n",
        "        training_time = format_time(time.time() - t0)\n",
        "        \n",
        "        print(f\"  Average training loss: {avg_train_loss:.3f}\")\n",
        "        print(f\"  Training epoch took: {training_time}\")\n",
        "        \n",
        "        # ========================================\n",
        "        #               Evaluation\n",
        "        # ========================================\n",
        "        print(\"Running Evaluation...\")\n",
        "        \n",
        "        t0 = time.time()\n",
        "        classifier_model.eval()\n",
        "        \n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "        \n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        \n",
        "        # Evaluate on test set\n",
        "        for batch in test_dataloader:\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                logits = classifier_model(b_input_ids, attention_mask=b_input_mask)\n",
        "                loss = criterion(logits, b_labels)\n",
        "                \n",
        "                # Calculate predictions\n",
        "                predictions = torch.argmax(logits, dim=-1)\n",
        "                \n",
        "                # Move to CPU for metric calculation\n",
        "                predictions = predictions.detach().cpu().numpy()\n",
        "                labels = b_labels.detach().cpu().numpy()\n",
        "                \n",
        "                # Store predictions and labels\n",
        "                all_preds.extend(predictions)\n",
        "                all_labels.extend(labels)\n",
        "                \n",
        "                # Calculate batch accuracy\n",
        "                batch_accuracy = accuracy_score(labels, predictions)\n",
        "                total_eval_accuracy += batch_accuracy\n",
        "                total_eval_loss += loss.item()\n",
        "                nb_eval_steps += 1\n",
        "        \n",
        "        # Calculate final metrics\n",
        "        avg_eval_accuracy = total_eval_accuracy / nb_eval_steps\n",
        "        avg_eval_loss = total_eval_loss / nb_eval_steps\n",
        "        eval_time = format_time(time.time() - t0)\n",
        "        \n",
        "        print(f\"  Accuracy: {avg_eval_accuracy:.3f}\")\n",
        "        print(f\"  Average evaluation loss: {avg_eval_loss:.3f}\")\n",
        "        print(f\"  Evaluation took: {eval_time}\")\n",
        "        \n",
        "        # Detailed classification report\n",
        "        print(\"\\nDetailed Classification Report:\")\n",
        "        print(classification_report(all_labels, all_preds, target_names=label_list, zero_division=0))\n",
        "        \n",
        "        # Confusion Matrix\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(confusion_matrix(all_labels, all_preds))\n",
        "        \n",
        "        # Record statistics\n",
        "        training_stats.append({\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_eval_loss,\n",
        "            'Valid. Accur.': avg_eval_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Evaluation Time': eval_time\n",
        "        })\n",
        "    else:\n",
        "        print(\"No successful training batches this epoch\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "print(f\"Total training took {format_time(time.time() - total_t0)}\")\n",
        "\n",
        "# Print training statistics\n",
        "if training_stats:\n",
        "    import pandas as pd\n",
        "    df_stats = pd.DataFrame(data=training_stats)\n",
        "    print(\"\\nTraining Statistics:\")\n",
        "    print(df_stats)\n",
        "    \n",
        "    # Compare with your GAN results\n",
        "    best_baseline_acc = max([stat['Valid. Accur.'] for stat in training_stats])\n",
        "    print(f\"\\nBaseline Transformer Best Accuracy: {best_baseline_acc:.3f}\")\n",
        "    print(f\"Your GAN Training Best Accuracy: 0.938\")\n",
        "    print(f\"GAN Improvement: {0.938 - best_baseline_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "GANBERT_pytorch croce.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "harshitml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0097aa33393342cd99be3dcc30edc5a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_504e8c3a107e4e04b51df39e1b3c584e",
            "max": 435779157,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a56008dc6c6b41c3850611cc15fb6ea8",
            "value": 435779157
          }
        },
        "098f203f1209452a9d4192af92da7057": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b94243bfea7e4441b809b38c7cecd875",
              "IPY_MODEL_0097aa33393342cd99be3dcc30edc5a0",
              "IPY_MODEL_293c4d8660f546f79b43e0dc63250c2f"
            ],
            "layout": "IPY_MODEL_ec76cfd2d1da498e9b66885ff1be46b3"
          }
        },
        "0dccd0ab28c34880be92b35aa05e6ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14a6abbb244b41f89626a37640c63118": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14bf2ab82bfc45bd8a3b93f2ab9aa656": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "193a5a054d6f4a319842820c4c308322": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b465cf9004f549ffa85444bfa16ee4c2",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7bef2816920414f99a5c9cc676ec254",
            "value": 213450
          }
        },
        "1f25240f5457445795af70e20e5903f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22cde8fbae4e49af993e33f3f2d9a28e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44660941ffcc44beae72a1974d458583",
              "IPY_MODEL_afdbd837001c4c74b5ac332ca061ef3a",
              "IPY_MODEL_b489404bf53b4fab9bdb1c0c79a33008"
            ],
            "layout": "IPY_MODEL_2c671428c4df4355a7a53c71e9bd14ee"
          }
        },
        "28f045edfa48462fa96a94cffd3b143f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "293c4d8660f546f79b43e0dc63250c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5110d1cb9a7547f49244113dc5dd8321",
            "placeholder": "​",
            "style": "IPY_MODEL_1f25240f5457445795af70e20e5903f9",
            "value": " 436M/436M [00:29&lt;00:00, 14.8MB/s]"
          }
        },
        "2bfb43b9e8604cbf8ebfd162267f1b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c671428c4df4355a7a53c71e9bd14ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30e0829529874db1802f411f72f3d76a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f41b83ccf26c40ed88ca6eaf49e09de5",
            "max": 435797,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0f0d9a0d4f4440e81e5a6d5a2a3b4ab",
            "value": 435797
          }
        },
        "44660941ffcc44beae72a1974d458583": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f556917850542da8508f9f839cae9bc",
            "placeholder": "​",
            "style": "IPY_MODEL_2bfb43b9e8604cbf8ebfd162267f1b8a",
            "value": "Downloading: 100%"
          }
        },
        "4d22913664fb4cdbb38e217d4197601d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f556917850542da8508f9f839cae9bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "504e8c3a107e4e04b51df39e1b3c584e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5110d1cb9a7547f49244113dc5dd8321": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "547d7329b8554b7bb8ae51d61a5e41f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bac8f28c84144450b24bbc97fa4860db",
              "IPY_MODEL_30e0829529874db1802f411f72f3d76a",
              "IPY_MODEL_83dd72fbb4204fefb951b3800d73a8d2"
            ],
            "layout": "IPY_MODEL_81b6bafd3fc248afa76cf463f2cb8ab8"
          }
        },
        "5a478b81997c4263a60d938447232fd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d501eb3d36e48128a5232879141b281",
            "placeholder": "​",
            "style": "IPY_MODEL_748d5c1fb00e4d91809003527319a9f6",
            "value": " 213k/213k [00:00&lt;00:00, 102kB/s]"
          }
        },
        "5fbebd67d40e470e8a75a4b5b540bbdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "748d5c1fb00e4d91809003527319a9f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d501eb3d36e48128a5232879141b281": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81b6bafd3fc248afa76cf463f2cb8ab8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83dd72fbb4204fefb951b3800d73a8d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b79fef6d585843c0a4d885edb2d01ebd",
            "placeholder": "​",
            "style": "IPY_MODEL_be6339f6256d4b579c53ef8b430ecb25",
            "value": " 436k/436k [00:00&lt;00:00, 1.26MB/s]"
          }
        },
        "870fe8f58bb24a47b6a98fa0eed0ebf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8b619dff35e4f8cabf586221ad8c962",
            "placeholder": "​",
            "style": "IPY_MODEL_5fbebd67d40e470e8a75a4b5b540bbdf",
            "value": "Downloading: 100%"
          }
        },
        "9fc9479d78e442db91360de7f45b6d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a56008dc6c6b41c3850611cc15fb6ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afdbd837001c4c74b5ac332ca061ef3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14a6abbb244b41f89626a37640c63118",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28f045edfa48462fa96a94cffd3b143f",
            "value": 570
          }
        },
        "b465cf9004f549ffa85444bfa16ee4c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b489404bf53b4fab9bdb1c0c79a33008": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf127a02cf6647aba4035c5cbfadc378",
            "placeholder": "​",
            "style": "IPY_MODEL_0dccd0ab28c34880be92b35aa05e6ffb",
            "value": " 570/570 [00:00&lt;00:00, 12.0kB/s]"
          }
        },
        "b79fef6d585843c0a4d885edb2d01ebd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b94243bfea7e4441b809b38c7cecd875": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14bf2ab82bfc45bd8a3b93f2ab9aa656",
            "placeholder": "​",
            "style": "IPY_MODEL_9fc9479d78e442db91360de7f45b6d7f",
            "value": "Downloading: 100%"
          }
        },
        "bac8f28c84144450b24bbc97fa4860db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddf20490dc874352aa7df35f07a60bcc",
            "placeholder": "​",
            "style": "IPY_MODEL_f9390d4fc9b147729f1ef5ea85d03774",
            "value": "Downloading: 100%"
          }
        },
        "be6339f6256d4b579c53ef8b430ecb25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf127a02cf6647aba4035c5cbfadc378": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0f0d9a0d4f4440e81e5a6d5a2a3b4ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3a13b7869354881ac7b7887e05d56a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_870fe8f58bb24a47b6a98fa0eed0ebf5",
              "IPY_MODEL_193a5a054d6f4a319842820c4c308322",
              "IPY_MODEL_5a478b81997c4263a60d938447232fd9"
            ],
            "layout": "IPY_MODEL_4d22913664fb4cdbb38e217d4197601d"
          }
        },
        "d7bef2816920414f99a5c9cc676ec254": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8b619dff35e4f8cabf586221ad8c962": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddf20490dc874352aa7df35f07a60bcc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec76cfd2d1da498e9b66885ff1be46b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f41b83ccf26c40ed88ca6eaf49e09de5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9390d4fc9b147729f1ef5ea85d03774": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
